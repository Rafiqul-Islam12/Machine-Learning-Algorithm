# ***Curse of Dimensionality***

---
- ***Feature (dimension) ‡¶Ø‡¶§ ‡¶¨‡¶æ‡ßú‡ßá, data ‡¶§‡¶§ sparse ‡¶π‡ßü‡ßá ‡¶Ø‡¶æ‡ßü,    
‡¶Ü‡¶∞ model ‡¶è‡¶∞ ‡¶∂‡ßá‡¶ñ‡¶æ ‡¶§‡¶§ ‡¶ï‡¶†‡¶ø‡¶® ‡¶π‡ßü ‚Äî ‡¶è‡¶ü‡¶æ‡¶á Curse of Dimensionality‡•§***  

***Sparse = Data point ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶è‡¶ï ‡¶ú‡¶æ‡ßü‡¶ó‡¶æ‡ßü ‡¶ú‡¶Æ‡ßá ‡¶®‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¶‡ßÇ‡¶∞‡ßá ‡¶¶‡ßÇ‡¶∞‡ßá ‡¶õ‡ßú‡¶ø‡ßü‡ßá ‡¶•‡¶æ‡¶ï‡ßá ‚Üí ‡¶è‡¶ü‡¶æ‡¶ï‡ßá‡¶á ‡¶¨‡¶≤‡ßá sparse data***   

***‡¶Ø‡¶ñ‡¶® dataset-‡¶è‡¶∞ feature (dimension) ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ñ‡ßÅ‡¶¨ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶π‡ßü‡ßá ‡¶Ø‡¶æ‡ßü, ‡¶§‡¶ñ‡¶® data space ‡¶è‡¶§ ‡¶¨‡ßú ‡¶π‡ßü‡ßá ‡¶Ø‡¶æ‡ßü ‡¶Ø‡ßá ‡¶∏‡ßÄ‡¶Æ‡¶ø‡¶§ data ‡¶¶‡¶ø‡ßü‡ßá machine learning model ‡¶≠‡¶æ‡¶≤‡ßã‡¶≠‡¶æ‡¶¨‡ßá pattern ‡¶∂‡¶ø‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá ‡¶®‡¶æ‡•§***  
- ***1 feature ‚Üí 1D***
- ***2 feature ‚Üí 2D***
- ***10+ feature ‚Üí High-Dimensional space***   
***Feature ‡¶Ø‡¶§ ‡¶¨‡¶æ‡ßú‡ßá, space ‡¶§‡¶§ exponentially ‡¶¨‡ßú ‡¶π‡ßü***
 
üîπüîπüîπ     
***High dimension ‡¶è Distance Meaningless ‡¶π‡ßü‡ßá ‡¶Ø‡¶æ‡ßü  
‡¶∏‡¶¨ point-‡¶è‡¶∞ ‡¶¶‡ßÇ‡¶∞‡¶§‡ßç‡¶¨ ‡¶™‡ßç‡¶∞‡¶æ‡ßü ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶π‡ßü‡ßá ‡¶Ø‡¶æ‡ßü   
‡¶Æ‡¶æ‡¶®‡ßá:   
Nearest neighbor ‚âà Farthest neighbor   
‚Äú‡¶ï‡¶æ‡¶õ‡ßá‡¶∞ point‚Äù ‡¶¨‡¶≤‡ßá ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶•‡¶æ‡¶ï‡ßá ‡¶®‡¶æ   
‡¶§‡¶æ‡¶á distance-based algorithm fail ‡¶ï‡¶∞‡ßá***   
  
***Example: KNN   
Low dimension ‚Üí nearest neighbor ‡¶∏‡¶§‡ßç‡¶Ø‡¶ø‡¶á ‡¶ï‡¶æ‡¶õ‡ßá‡¶∞  
High dimension ‚Üí ‡¶∏‡¶¨ neighbor ‡¶™‡ßç‡¶∞‡¶æ‡ßü same ‡¶¶‡ßÇ‡¶∞‡¶§‡ßç‡¶¨‡ßá***  
- ***KNN confuse ‡¶π‡ßü***
- ***Accuracy ‡¶ï‡¶Æ‡ßá ‡¶Ø‡¶æ‡ßü***

‚ö†Ô∏è    
***Highly affected   
KNN    
K-means   
DBSCAN   
SVM (high-dim kernel)     
Decision Tree   
Random Forest***  

üîπüîπüîπ   
***Curse Dimensionality ‡¶¶‡ßÇ‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶â‡¶™‡¶æ‡ßü***  
1. ***Dimensionality Reduction   
PCA  
LDA  
Autoencoder***  
2. ***Feature Selection  
Unimportant feature ‡¶¨‡¶æ‡¶¶  
Correlated feature remove***   
3. ***More Data  
Feature ‡¶¨‡¶æ‡ßú‡¶æ‡¶≤‡ßá  
Data ‡¶ì ‡¶¨‡¶æ‡ßú‡¶æ‡¶§‡ßá ‡¶π‡¶¨‡ßá***   

---
# ***Principal Component Analysis (PCA)***  

---
- ***Dimensionality Reduction technique***
- ***‡¶¨‡ßá‡¶∂‡¶ø feature (dimension) ‡¶•‡¶æ‡¶ï‡¶æ dataset ‡¶•‡ßá‡¶ï‡ßá ‡¶ï‡¶Æ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶ï ‡¶®‡¶§‡ßÅ‡¶® feature ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá,   
‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ø‡¶§‡¶ü‡¶æ ‡¶∏‡¶Æ‡ßç‡¶≠‡¶¨ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ information (variance) ‡¶ß‡¶∞‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ ‡¶π‡ßü‡•§***

- ***PCA ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞ beacuse-***  
***Feature Extraction  
Curse of Dimensionality handle ‡¶ï‡¶∞‡¶§‡ßá    
Model faster ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø   
Overfitting ‡¶ï‡¶Æ‡¶æ‡¶®‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø   
Visualization ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø     
Distance-based algorithm ‡¶†‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶æ‡¶®‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø   
Noise reduction ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø***

***PCA Finding:***
- ***‚Äú‡¶ï‡ßã‡¶® direction ‡¶è data project ‡¶ï‡¶∞‡¶≤‡ßá variation (spread) ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶•‡¶æ‡¶ï‡¶¨‡ßá?‚Äù***

--- 
# PCA: Step by Step

***`Step 1`: Standardize the Data      
‡¶∏‡¶¨ features ‡¶ï‡ßá mean 0, std 1 ‡¶è ‡¶Ü‡¶®‡¶æ ‡¶π‡ßü‡•§      
Scale mismatch ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶¨‡¶°‡¶º scale dominate ‡¶ï‡¶∞‡ßá, ‡¶§‡¶æ‡¶á standardize ‡¶ï‡¶∞‡¶æ ‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø‡•§***     
- ***‚Äú‡¶∏‡¶¨ feature ‡¶ï‡ßá same starting line ‡¶è ‡¶¶‡¶æ‡¶Å‡ßú ‡¶ï‡¶∞‡¶æ‡¶®‡ßã‡•§‚Äù***   

***`Step 2`: Compute Covariance Matrix***   
***Covariance matrix ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü, ‡¶Ø‡¶æ feature ‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ correlation ‡¶¶‡ßá‡¶ñ‡¶æ‡ßü‡•§    
‡¶ï‡ßã‡¶® feature ‡¶è‡¶ï‡ßá ‡¶Ö‡¶™‡¶∞‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶ï‡¶§‡ßã closely related, ‡¶∏‡ßá‡¶ü‡¶æ‡¶á ‡¶¨‡ßã‡¶ù‡¶æ‡•§***   

***`Step 3`: Compute Eigenvalues & Eigenvectors   
Covariance matrix ‡¶•‡ßá‡¶ï‡ßá eigenvalues ‡¶ì eigenvectors ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§***   
- ***Eigenvector ‚Üí direction (axis)***   
- ***Eigenvalue ‚Üí variance along that direction (importance)***    
- ***‡¶¨‡ßú eigenvalue ‚Üí ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßá‡¶∂‡¶ø information***   
- ***‡¶õ‡ßã‡¶ü eigenvalue ‚Üí noise / less important***   

***`Step 4`: Select Principal Components   
Eigenvalues descending order ‡¶è ‡¶∏‡¶æ‡¶ú‡¶æ‡¶®‡ßã ‡¶π‡ßü‡•§    
Top k eigenvectors (highest eigenvalues) ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§    
Tools: Scree plot (variance plot) ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá decision ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡ßü‡•§***   
- ***‚Äú‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá important info ‡¶™‡ßç‡¶∞‡¶•‡¶Æ component ‡¶è, ‡¶ï‡¶Æ important info ‡¶¨‡¶æ‡¶¶‡•§‚Äù***   

***`Step 5`: Form Feature Vector   
Top k eigenvectors ‡¶ï‡ßá ‡¶è‡¶ï‡¶ü‡¶ø matrix ‡¶Ü‡¶ï‡¶æ‡¶∞‡ßá ‡¶∏‡¶æ‡¶ú‡¶æ‡¶®‡ßã ‡¶π‡ßü‡•§   
‡¶è matrix ‡¶π‡¶ö‡ßç‡¶õ‡ßá Feature Vector‡•§    
‡¶è‡¶ü‡¶æ ‡¶∏‡ßá‡¶á axis ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ original data project ‡¶ï‡¶∞‡¶¨‡•§    
‡¶ï‡¶Æ dimension, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ maximum info retained‡•§***   

***`Step 6`: Transform Original Dataset   
Original data rotate ‡¶π‡ßü‡ßá ‡¶®‡¶§‡ßÅ‡¶® axis-‡¶è ‡¶ö‡¶≤‡ßá ‡¶Ü‡¶∏‡ßá‡•§   
Dimension reduce, information retained‡•§***   

***`Step 7`: Reconstruct Data   
‡¶™‡ßç‡¶∞‡¶•‡¶Æ few principal component ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá approximate original data ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§   
Low-variance component ‡¶¨‡¶æ‡¶¶ ‡¶¶‡¶ø‡¶≤‡ßá‡¶ì data almost same ‡¶•‡¶æ‡¶ï‡ßá‡•§  
Noise remove ‡¶π‡ßü‡•§***   

---
# ***Important Resources for Understaning Math***   
- [***Mathematical Approach to PCA in Geeksforgeeks***](https://www.geeksforgeeks.org/machine-learning/mathematical-approach-to-pca/)
- [***Awesome Mathematical Intuition***](https://medium.com/analytics-vidhya/the-math-of-principal-component-analysis-pca-bf7da48247fc)
- [***PCA And It‚Äôs Underlying Mathematical Principles***](https://www.analyticsvidhya.com/blog/2021/09/pca-and-its-underlying-mathematical-principles/)


